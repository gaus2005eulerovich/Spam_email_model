
import torch
import numpy as np
import os
import sys
import subprocess
import pandas as pd
import zipfile
import requests
import io
import inspect


# --- –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô ---
def install_and_upgrade():
    print("‚è≥ –û–±–Ω–æ–≤–ª—è–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–æ SOTA –≤–µ—Ä—Å–∏–π... –≠—Ç–æ –∑–∞–π–º–µ—Ç –º–∏–Ω—É—Ç—É.")
    subprocess.check_call(
        [sys.executable, "-m", "pip", "install", "--upgrade", "datasets", "transformers", "accelerate>=0.21.0",
         "scikit-learn", "pandas"])
    print("‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã! –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–Ω—É—Ç –æ—à–∏–±–∫–∏ –∏–º–ø–æ—Ä—Ç–∞, –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å—Ä–µ–¥—É (Runtime -> Restart Session).")


# –í—ã–∑—ã–≤–∞–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –≤–µ—Ä—Å–∏–π
try:
    import transformers
    # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä—Å–∏–∏, –µ—Å–ª–∏ –æ–Ω–∞ —Å–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä–∞—è - –æ–±–Ω–æ–≤–ª—è–µ–º
    from packaging import version

    if version.parse(transformers.__version__) < version.parse("4.42.0"):
        install_and_upgrade()
except ImportError:
    install_and_upgrade()

# –ò–º–ø–æ—Ä—Ç—ã –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
from datasets import load_dataset, Dataset
from transformers import (
    DistilBertTokenizer,
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# –û—Ç–∫–ª—é—á–∞–µ–º WandB
os.environ["WANDB_DISABLED"] = "true"


def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }


def load_data_fallback():
    print("‚ö†Ô∏è HuggingFace load failed. Trying direct download from UCI Repository...")
    url = "https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip"
    r = requests.get(url)
    z = zipfile.ZipFile(io.BytesIO(r.content))
    with z.open('SMSSpamCollection') as f:
        df = pd.read_csv(f, sep='\t', header=None, names=['label', 'sms'])
    df['label'] = df['label'].map({'ham': 0, 'spam': 1})
    dataset = Dataset.from_pandas(df)
    dataset = dataset.train_test_split(test_size=0.2)
    print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —á–µ—Ä–µ–∑ Fallback!")
    return dataset


def main():
    model_name = "distilbert-base-uncased"

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
    if torch.cuda.is_available():
        device = "cuda"
        print(f"‚úÖ GPU –Ω–∞–π–¥–µ–Ω: {torch.cuda.get_device_name(0)}")
    else:
        device = "cpu"
        print("‚ö†Ô∏è GPU –ù–ï –ù–ê–ô–î–ï–ù! –û–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω—ã–º.")
        print("–í–∫–ª—é—á–∏ GPU: Runtime -> Change runtime type -> T4 GPU")

    # --- –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ---
    try:
        dataset = load_dataset("sms_spam", split="train")
        dataset = dataset.train_test_split(test_size=0.2)
        print("‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —á–µ—Ä–µ–∑ HuggingFace.")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ HF: {e}")
        dataset = load_data_fallback()

    # --- –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø ---
    tokenizer = DistilBertTokenizer.from_pretrained(model_name)

    def preprocess_function(examples):
        return tokenizer(examples["sms"], truncation=True, padding=False)

    tokenized_datasets = dataset.map(preprocess_function, batched=True)
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # --- –ú–û–î–ï–õ–¨ ---
    id2label = {0: "HAM (–ù–æ—Ä–º)", 1: "SPAM (–°–ø–∞–º)"}
    label2id = {"HAM (–ù–æ—Ä–º)": 0, "SPAM (–°–ø–∞–º)": 1}

    model = DistilBertForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2,
        id2label=id2label,
        label2id=label2id
    ).to(device)

    # --- –ü–ê–†–ê–ú–ï–¢–†–´ ---
    init_args = inspect.signature(TrainingArguments.__init__).parameters
    eval_strategy_key = "eval_strategy" if "eval_strategy" in init_args else "evaluation_strategy"
    print(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: {eval_strategy_key}")

    # –°–æ–±–∏—Ä–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –≤ —Å–ª–æ–≤–∞—Ä—å
    args_dict = {
        "output_dir": "./results",
        "learning_rate": 2e-5,
        "per_device_train_batch_size": 16,
        "per_device_eval_batch_size": 16,
        "num_train_epochs": 2,
        "weight_decay": 0.01,
        "save_strategy": "epoch",
        "load_best_model_at_end": True,
        "push_to_hub": False,
        "report_to": "none",
        eval_strategy_key: "epoch"  # –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫–ª—é—á –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏
    }

    training_args = TrainingArguments(**args_dict)

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    trainer.train()

    save_path = "./my_spam_model"
    model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)
    print(f"üèÅ –ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫—É {save_path}.")


if __name__ == "__main__":
    main()


